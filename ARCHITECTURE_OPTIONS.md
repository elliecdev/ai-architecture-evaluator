### 1️⃣ Supported Architectures (v1)

## **Prompt-only**

**What it is**

A single LLM call using a carefully designed prompt, relying entirely on the model’s pre-trained knowledge and the provided instructions.

**When it’s appropriate**

- Low-risk use cases
- Generic knowledge or summarization
- Rapid prototyping or internal tools
- When cost and latency must be minimal

**Main downside**

- No grounding in source data
- Higher risk of hallucinations
- Limited control over factual accuracy

---

## **Simple RAG (Retrieval-Augmented Generation)**

**What it is**

A two-step approach where relevant documents are retrieved via embeddings and included as context in the prompt before generating a response.

**When it’s appropriate**

- Answering questions from internal documentation
- Use cases requiring factual grounding
- Moderate accuracy requirements
- Static or slowly changing knowledge bases

**Main downside**

- Increased cost and latency
- Performance depends heavily on chunking and retrieval quality
- Can still hallucinate if retrieval is weak

---

## **RAG with Re-ranking**

**What it is**

An extension of simple RAG where retrieved documents are scored and reordered to prioritize the most relevant context before generation.

**When it’s appropriate**

- Larger document sets
- Higher accuracy requirements
- Scenarios where irrelevant context degrades answer quality

**Main downside**

- Additional complexity and cost
- More moving parts to tune and monitor
- Diminishing returns for small datasets

---

## **Multi-step Reasoning (Chained Prompts)**

**What it is**

A workflow where the LLM performs multiple sequential steps (e.g., analysis → synthesis → validation) rather than producing a single response.

**When it’s appropriate**

- Complex reasoning tasks
- Structured analysis or decision support
- Situations where intermediate reasoning improves quality

**Main downside**

- Higher latency and token usage
- More failure modes
- Harder to debug and evaluate consistently

---

### 2️⃣ What We Intentionally Exclude (v1)

## **Fine-tuned Models**

**Why excluded**

Fine-tuning introduces operational complexity, longer iteration cycles, and ongoing maintenance costs. For most internal decision-support use cases, prompt engineering and retrieval provide sufficient quality with far lower overhead.

**EM signal:** build vs buy thinking.

---

## **Autonomous Agents**

**Why excluded**

Agentic systems increase unpredictability, cost, and failure modes. This platform focuses on *decision support*, not autonomous action, and prioritizes explainability and control over novelty.

**EM signal:** risk containment.

---

## **Tool-Calling Workflows**

**Why excluded**

While powerful, tool calling adds orchestration complexity and blurs responsibility between the model and the system. It is intentionally deferred to keep architecture comparisons focused on LLM behavior rather than integration logic.

**EM signal:** scope discipline.

---

## **Real-Time Web Search**

**Why excluded**

Live data retrieval introduces reliability, compliance, and reproducibility concerns. This platform prioritizes deterministic evaluation using controlled, known datasets.

**EM signal:** governance and auditability.

---

# 3️⃣ Common Interface Contract (Conceptual)

To enable consistent comparison across different LLM architectures, all approaches adhere to a shared input and output contract. This ensures evaluation focuses on architectural tradeoffs rather than implementation details.

---

## **Input Fields**

### **Question**

The user’s primary query or task to be completed by the LLM.

- Expressed in natural language
- Identical across all architecture evaluations
- Represents the real business problem being solved

**Rationale:**

Keeping the question constant ensures differences in output are driven by architecture, not prompt phrasing.

---

### **Documents**

A collection of reference materials used to ground responses.

- Optional (not used by prompt-only approaches)
- May include internal documentation, policies, or knowledge base content
- Pre-processed into chunks prior to retrieval

**Rationale:**

Explicitly separating documents from the question clarifies when and how grounding is applied.

---

### **Constraints**

Business and technical requirements that influence architectural decisions.

Examples:

- Accuracy requirements (low / medium / high)
- Risk tolerance
- Cost sensitivity
- Latency expectations

**Rationale:**

Constraints allow the platform to recommend architectures aligned with business priorities rather than purely technical performance.

---

## **Output Fields**

### **Answer**

The final response generated by the selected architecture.

- Intended for human consumption
- Structured or unstructured depending on the use case
- Consistent formatting across architectures when possible

**Rationale:**

This is the primary artifact evaluated by users and reviewers.

---

### **Citations**

References to source documents used to support the answer.

- Included when documents are provided
- Mapped to specific document chunks
- Omitted or explicitly marked as unavailable for prompt-only responses

**Rationale:**

Citations improve trust, transparency, and debuggability.

---

### **Cost**

Estimated cost of generating the response.

- Based on token usage and model pricing
- Reported per request
- Used for architectural comparison, not billing accuracy

**Rationale:**

Cost visibility enables informed tradeoffs and prevents unintentional overspend.

---

### **Latency**

Time taken to generate the response.

- Measured end-to-end
- Includes retrieval and processing steps
- Reported in milliseconds or seconds

**Rationale:**

Latency is a critical factor in user experience and system scalability.

---

### **Confidence**

An indicator of response reliability.

- Derived from heuristics (e.g., self-assessment, citation coverage, consistency checks)
- Not a guarantee of correctness
- Used to flag responses requiring human review

**Rationale:**

Confidence signals help manage risk without overstating model certainty.

---

## 4️⃣ First-Order Tradeoffs Table (Rough)

> These values represent initial hypotheses based on expected behavior and will be validated through implementation and evaluation.
> 

| Architecture | Accuracy | Cost | Latency | Risk |
| --- | --- | --- | --- | --- |
| Prompt-only | Low | Low | Low | High |
| Simple RAG | Medium | Medium | Medium | Medium |
| RAG + Re-ranking | High | High | High | Low |
| Multi-step Reasoning | High | High | High | Medium |

---

### Reasoning

### **Prompt-only**

- **Accuracy: Low** — no grounding, relies on model memory
- **Cost: Low** — single call, minimal tokens
- **Latency: Low** — fastest path
- **Risk: High** — hallucinations and unverifiable claims

---

### **Simple RAG**

- **Accuracy: Medium** — grounded but retrieval quality varies
- **Cost: Medium** — embeddings + retrieval + generation
- **Latency: Medium** — extra steps
- **Risk: Medium** — reduced hallucinations, but not eliminated

---

### **RAG + Re-ranking**

- **Accuracy: High** — better context relevance improves answers
- **Cost: High** — additional scoring pass
- **Latency: High** — more computation per request
- **Risk: Low** — strongest grounding and traceability

---

### **Multi-step Reasoning**

- **Accuracy: High** — improved reasoning and structure
- **Cost: High** — multiple LLM calls
- **Latency: High** — sequential execution
- **Risk: Medium** — reasoning errors compound across steps